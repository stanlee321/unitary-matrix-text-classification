{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanlee321/unitary-matrix-text-classification/blob/master/QuantumFunctions_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8NHJOJn3D3F"
      },
      "source": [
        "# Unitary Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phkTKCRW3XsW"
      },
      "source": [
        "## 2. Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sIDI-lfLoXnc"
      },
      "outputs": [],
      "source": [
        "# Some imports\n",
        "\n",
        "from scipy.linalg import fractional_matrix_power as MatrixPow\n",
        "from scipy.stats import unitary_group\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84hik5LWq1nG"
      },
      "outputs": [],
      "source": [
        "## Setup Custom functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nvGel9xNomrL"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "def calculate_epsilon_simple(count_dict,\n",
        "                            word,\n",
        "                            unk_e: float = 0.000001,\n",
        "                            lf_e: float = 0.0001,\n",
        "                            hf_e: float = 0.001,\n",
        "                            lf_range: int = 0,\n",
        "                            hf_range: int = 1250):\n",
        "    try:\n",
        "        count = count_dict[word]\n",
        "    except:\n",
        "        return unk_e, \"unk\"\n",
        "    \n",
        "    if lf_range <= count <= hf_range:\n",
        "        return lf_e, \"lf\"\n",
        "    return hf_e, \"hf\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YIxab5rvoZz5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_unique_words_with_epsilon(\n",
        "    vocav_frec,     # Vocab used\n",
        "    unique_words,   # List of unique words\n",
        "    lf_e: float=0.0001,\n",
        "    hf_e: float=0.001,\n",
        "    lf_range: int=0,\n",
        "    hf_range: int=1250):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    w_e_pairs_list = []\n",
        "\n",
        "    for w in tqdm(unique_words):\n",
        "\n",
        "        e, kind_word = calculate_epsilon_simple(\n",
        "                vocav_frec,\n",
        "                w,\n",
        "                lf_e = lf_e,\n",
        "                hf_e = hf_e,\n",
        "                lf_range = lf_range,\n",
        "                hf_range = hf_range\n",
        "        )\n",
        "        w_e_pairs_list.append( (w, e, kind_word ) )\n",
        "\n",
        "    return w_e_pairs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "umK4HdJ1qYh8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def RandomUnitaryBaseElement(n,epsilon):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        n: Matrix size\n",
        "        epsilon: small mumber where zero limits to the identity matrix\n",
        "    \"\"\"\n",
        "    return MatrixPow( unitary_group.rvs(n), epsilon )\n",
        "\n",
        "def QuasiDiagonalBasisElement(n,epsilon, dtheta,i):\n",
        "    diag1 = np.roll( np.append(np.ones(n-1), np.exp(1j*dtheta)) ,1)\n",
        "    return np.diag(np.roll(diag1,i)).dot(RandomUnitaryBaseElement(n,epsilon)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "30dhychbqms9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def flatten_list(L):\n",
        "        return list( itertools.chain.from_iterable( L ) )\n",
        "\n",
        "def Grid_dU_Diagonal(n,epsilon):\n",
        "    axis0 = np.array([epsilon]+[0]*(n-1))\n",
        "    \n",
        "    return [ np.diag( np.exp(1j*epsilon*np.roll(axis0,n)) ) for n in range(n) ]\n",
        "\n",
        "\n",
        "def CanonicalCosetBase(X_01):\n",
        "    \"\"\"\n",
        "    Constructs a canonical coset matrix from the row vector X_01\n",
        "    \"\"\"\n",
        "    m = len(X_01) \n",
        "    X_01 = np.array([X_01])\n",
        "    X_00 = np.sqrt( 1-np.real(X_01@X_01.conj().T)  )\n",
        "    X_11 = linalg.sqrtm( np.eye(m) - X_01.conj().T@X_01 )\n",
        "    \n",
        "    return np.block( [ [ X_00          , X_01],\n",
        "                       [-X_01.T.conj() , X_11]] ) \n",
        "\n",
        "def CanonicalCoset(X,n):\n",
        "    \"\"\"\n",
        "    Constructs a canonical coset matrix from the row vector X_01 embedded in a matrix space nxn\n",
        "    \"\"\"\n",
        "    m = len(X)\n",
        "    \n",
        "    if m==n-1:\n",
        "        return CanonicalCosetBase(X)\n",
        "    \n",
        "    identity = np.eye(n-m-1)\n",
        "    \n",
        "    zero = [0]*(len(X)+1)\n",
        "    zero = np.array([ zero for _ in range(len(identity))])    \n",
        "    \n",
        "    #return CanonicalCosetBase(X) \n",
        "    \n",
        "    return np.block([ [identity,zero],\n",
        "                      [zero.T, CanonicalCosetBase(X) ] ])\n",
        "\n",
        "\n",
        "def Grid_dU_CanonicalCoset( n, m ,epsilon ):\n",
        "    \"\"\"\n",
        "    Left Canonical Coset m, for nxn unitary matrices \n",
        "    \"\"\"\n",
        "    axis0 = np.array([epsilon]+[0]*(m-1))\n",
        "    \n",
        "    axis_list = np.array([ np.roll(axis0,k) for k in range(m) ] )\n",
        "    \n",
        "    pre = [  [ CanonicalCoset( epsilon*X , n ), CanonicalCoset( 1j*epsilon*X , n ) ] for X in axis_list ]\n",
        "    \n",
        "    return flatten_list(pre)\n",
        "\n",
        "\n",
        "def Grid_dU(n, epsilon ):\n",
        "    \n",
        "    CCosets_list = [  Grid_dU_CanonicalCoset( n, m ,epsilon ) for m in  range( n-1, 0, -1 ) ]\n",
        "    \n",
        "    return flatten_list(CCosets_list) + Grid_dU_Diagonal(n,epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7ff8QoUOpBTZ"
      },
      "outputs": [],
      "source": [
        "def Text_DiagonalBaseDict(n, w_e_pairs_list, dtheta, epsilon_grid):\n",
        "    \"\"\"\n",
        "    This basis samples random matrices around the diagonal\n",
        "    \"\"\"\n",
        "\n",
        "    # Create dequeue for els in grid\n",
        "    deq = collections.deque( Grid_dU(n, epsilon_grid) )\n",
        "\n",
        "    basis = {}\n",
        "\n",
        "    for i, (w, epsilon, kind) in enumerate(tqdm(w_e_pairs_list)):       \n",
        "        if kind == \"lf\":\n",
        "            if len(deq) > 0:\n",
        "                basis[w] = deq.pop()\n",
        "            else:\n",
        "                basis[w] = QuasiDiagonalBasisElement(n,epsilon, dtheta,i)\n",
        "        else:\n",
        "            basis[w] = QuasiDiagonalBasisElement(n,epsilon,dtheta,i)\n",
        "    \n",
        "    return basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAAGNpA4BLRt"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "#########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2c-EV3SFAiH8"
      },
      "outputs": [],
      "source": [
        "def HouseholderLeftDecomposition(W):\n",
        "    H = W.copy()  \n",
        "    n = H.shape[0]\n",
        "\n",
        "    ee = np.identity(n)\n",
        "\n",
        "    factor_list = []\n",
        "\n",
        "    for i in range(H.shape[0]-1):\n",
        "\n",
        "        col = H[:,i]\n",
        "        phase = np.angle(col[i])\n",
        "        u = np.array([col + np.exp(1j*phase)*ee[i] ])\n",
        "        uu = u.conj().T.dot(u)\n",
        "        Q = (ee - 2*uu/np.real(np.trace(uu))).T\n",
        "        H = Q.dot(H)\n",
        "        factor_list.append(Q)\n",
        "\n",
        "    factor_list.append(H)\n",
        "\n",
        "    return factor_list\n",
        "\n",
        "def CanonicalCosetLeftDecomposition(W):\n",
        "    \n",
        "    def ReflectionMatrix(n,i):\n",
        "        if i < n-1:\n",
        "            r = np.identity(n)\n",
        "            r[i,i] = -1\n",
        "            return r\n",
        "        elif i == n-1:\n",
        "            r = -np.identity(n)\n",
        "            r[i,i] = 1\n",
        "            return r\n",
        "    \n",
        "    n = W.shape[0]\n",
        "    \n",
        "    factors = HouseholderLeftDecomposition(W)\n",
        "    \n",
        "    factors = [ F@ReflectionMatrix(n,i) for i,F in enumerate(factors) ]\n",
        "    \n",
        "    return factors\n",
        "\n",
        "\n",
        "\n",
        "# The trick is to extract the independent parameters from each Householder matrix\n",
        "\n",
        "def CanonicalCosetPosition(C,i):\n",
        "    n = C.shape[0]\n",
        "    if i < n-1:\n",
        "        return C[i,i+1:]\n",
        "    elif i == n-1:\n",
        "        return np.diag(C)\n",
        "\n",
        "\n",
        "def CanonicalCosetLeftDecompositionParametersComplex(W):\n",
        "    factors = CanonicalCosetLeftDecomposition(W)\n",
        "    \n",
        "    positions = [ CanonicalCosetPosition(f,i) for i,f in enumerate(factors) ]\n",
        "    \n",
        "    return positions\n",
        "\n",
        "def CanonicalCosetLeftDecompositionParametersReal(W):\n",
        "    \n",
        "    def CanonicalCosetPosition(C,i):\n",
        "        n = C.shape[0]\n",
        "        if i<n-1:\n",
        "            return C[i,i+1:]\n",
        "        elif i == n-1:\n",
        "            return np.diag(C)\n",
        "    \n",
        "    n = W.shape[0]\n",
        "    \n",
        "    factors = CanonicalCosetLeftDecomposition(W)\n",
        "    \n",
        "    positions = np.concatenate(\n",
        "        [ CanonicalCosetPosition(f,i) for i,f in enumerate(factors) if i<n-1 ])\n",
        "    \n",
        "    # positions = np.concatenate([ np.real(positions) , np.imag(positions) ])\n",
        "    # phases = np.real(np.angle( np.diag(factors[n-1]) ))\n",
        "\n",
        "    positions = np.array([ (np.real(x),np.imag(x)) for x in positions  ]).flatten()\n",
        "    phases = np.real(np.angle( np.diag(factors[n-1]) ))\n",
        "    \n",
        "    \n",
        "    return  np.concatenate( [positions,phases] )\n",
        "\n",
        "\n",
        "## create function for create the composite matrix\n",
        "#  we multiply the sequence of matrices\n",
        "# and we obtain a final composite matrix\n",
        "\n",
        "def create_composite_matrix(text_to_matrix_dict:dict , input_tokens:list):\n",
        "\n",
        "    M = text_to_matrix_dict[ input_tokens[0] ]\n",
        "\n",
        "    for a in input_tokens:\n",
        "        M = M.dot(text_to_matrix_dict[a])\n",
        "\n",
        "    return M\n",
        "\n",
        "def create_embeding_matrix(words_to_unitary_dict:dict, text_inputs:list ):\n",
        "    \"\"\"\n",
        "    Creates the embeding representation for some list of texts inputs. \n",
        "        e.g. : text_inputs = [\"this, \"is\", \"one\", \"example\"]\n",
        "    \"\"\"\n",
        "\n",
        "    # Create composite matrix for token inputs \n",
        "    W = create_composite_matrix(words_to_unitary_dict, input_tokens = text_inputs)\n",
        "\n",
        "    # Create embeding\n",
        "    E = CanonicalCosetLeftDecompositionParametersReal(W)\n",
        "\n",
        "    return E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nunjLWBM5FEk"
      },
      "source": [
        "## Text Classification Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5pJGd3mtpKwi"
      },
      "outputs": [],
      "source": [
        "### Setup text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRpnPgdQpHfA",
        "outputId": "d866d76d-4b98-4f8c-e639-d7833dca3f75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0f9ee31250>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "#from torchtext.datasets import AG_NEWS\n",
        "# import datasets\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "from torch.autograd import Function\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui1DarPSrGKK",
        "outputId": "bfb843d7-cfdf-45b3-a407-8612ff0fa9ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/stanley/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgFn_UxdrKa3",
        "outputId": "3be0c410-28c1-415f-a32b-d0e4a606c09f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz3NtwI9rNn3",
        "outputId": "f4aa0ada-0a8d-4e0a-9fb1-270e099238e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stanley/anaconda3/envs/ML_p38/lib/python3.8/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n",
            "/home/stanley/anaconda3/envs/ML_p38/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "#train_iter, val_iter = AG_NEWS()\n",
        "\n",
        "train_iter, val_iter = IMDB()\n",
        "\n",
        "#next(train_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xZ-mvU-Lsmeb"
      },
      "outputs": [],
      "source": [
        "\n",
        "labels = []\n",
        "counter = Counter()\n",
        "sizes = []\n",
        "\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "    labels.append(label)\n",
        "    sizes.append(len(line))\n",
        "    \n",
        "for (label, line) in val_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "    labels.append(label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rW5WvWTErQ0f"
      },
      "outputs": [],
      "source": [
        "# load again the dataset because of the generator used before\n",
        "train_iter, val_iter = IMDB()\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter + val_iter), specials=[\"<unk>\"])\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxr3ZY7SrTOZ",
        "outputId": "c7e2a968-179f-4c63-b445-384befdbb32d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "147157\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "print(VOCAB_SIZE)\n",
        "UNIQUE_WORDS = vocab.get_itos()\n",
        "FREC_DICT =  dict(counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yt-7a4ursMY",
        "outputId": "5c2fe27b-8bd5-47d4-a363-5a4c13bdd33f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 147157/147157 [00:00<00:00, 1348710.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<unk>', 1e-06, 'unk'), ('the', 0.001, 'hf'), ('.', 0.001, 'hf'), (',', 0.001, 'hf'), ('and', 0.001, 'hf'), ('a', 0.001, 'hf'), ('of', 0.001, 'hf'), ('to', 0.001, 'hf'), (\"'\", 0.001, 'hf'), ('is', 0.001, 'hf')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "w_e_pairs_simple = create_unique_words_with_epsilon(\n",
        "        FREC_DICT,\n",
        "        UNIQUE_WORDS,\n",
        "        lf_e=0.0001,\n",
        "        hf_e=0.001,\n",
        "        lf_range=0,\n",
        "        hf_range=825\n",
        "    )\n",
        "\n",
        "print(w_e_pairs_simple[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGEIz9kl0caU",
        "outputId": "0b66c281-111f-4944-ad7b-33309e4d2ac1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'unk': 1, 'hf': 1299, 'lf': 145857})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels_tokens = [sample[2] for sample in w_e_pairs_simple ]\n",
        "\n",
        "labels_tokens[:10]\n",
        "\n",
        "c = Counter(labels_tokens)\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr1kPgEp6UZs",
        "outputId": "f482f39d-ec61-445b-c43a-d29cd2edeb9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 147157/147157 [04:46<00:00, 514.08it/s]\n"
          ]
        }
      ],
      "source": [
        "N = 15\n",
        "theta = 0.001\n",
        "\n",
        "epsilon_grid = 0.001\n",
        "\n",
        "# Token to Unitary(N) map\n",
        "AA_dict = Text_DiagonalBaseDict( N, w_e_pairs_simple, theta, epsilon_grid )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kxRfsQRZ5C7u"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5cY6IUDC5C_4"
      },
      "outputs": [],
      "source": [
        "# Create tokenizer \n",
        "text_pipeline = lambda x: tokenizer(x)\n",
        "\n",
        "# Create labels for this kind of dataset\n",
        "label_pipeline = lambda x: 1 if x == \"pos\" else 0\n",
        "\n",
        "\n",
        "def convert_text_to_features(AA_dict, text):\n",
        "\n",
        "    text_tokens = text_pipeline(text)\n",
        "\n",
        "    embedding = create_embeding_matrix( words_to_unitary_dict = AA_dict, text_inputs = text_tokens  )\n",
        "\n",
        "    sample = {\"Text\": np.float32(embedding), \"Class\": label}\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "def create_features_label_df(input_dataframe, AA_dict:dict ):\n",
        "    \n",
        "    _df = input_dataframe.copy()\n",
        "    \n",
        "    # Calculate embeeded representation for text\n",
        "    _df[\"features\"] = _df[\"Text\"].progress_apply(\n",
        "        lambda x: convert_text_to_features(AA_dict, x)[\"Text\"])\n",
        "    \n",
        "    # Create features columns\n",
        "    features = _df[\"features\"].apply(pd.Series)\n",
        "    features = features.rename(columns = lambda x : 'feature_' + str(x))\n",
        "    new_df = pd.concat([_df[[\"Text\",\"Class\"]], features[:]], axis=1)\n",
        "\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKJkUxqf5d5j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Up8PKZ5ld8"
      },
      "source": [
        "Setup the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "emy_Knfw5DDV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stanley/anaconda3/envs/ML_p38/lib/python3.8/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n",
            "/home/stanley/anaconda3/envs/ML_p38/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
          ]
        }
      ],
      "source": [
        "# Load again the datset\n",
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "\n",
        "train_pairs = [{\"Text\": line, \"Class\": label} for (label, line) in split_train_]\n",
        "valid_pairs = [{\"Text\": line, \"Class\": label} for (label, line) in split_valid_]\n",
        "test_pairs  = [{\"Text\": line, \"Class\": label} for (label, line) in test_dataset]\n",
        "\n",
        "\n",
        "# create Pandas DataFrame\n",
        "text_labels_df_train = pd.DataFrame(train_pairs)\n",
        "text_labels_df_valid = pd.DataFrame(valid_pairs)\n",
        "text_labels_df_test = pd.DataFrame(test_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "zGDSk1AO0Crn",
        "outputId": "eb50a0f3-a72f-48c9-8568-ae9c936f2033"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Before I begin, I want to briefly say that thi...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>There has been a lot of love that has been put...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hello I am from Denmark, and one day i was hav...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Shot into car from through the windscreen, som...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For all of the hype about this film, I kept an...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23745</th>\n",
              "      <td>\"Godzilla vs King Ghidorah\" is a perfect examp...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23746</th>\n",
              "      <td>This is one of those films that explore the cu...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23747</th>\n",
              "      <td>Ok I will sum up this movie... A bunch of skan...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23748</th>\n",
              "      <td>Wow-this one sucks. I'm gonna sum it up as qui...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23749</th>\n",
              "      <td>I suppose it's nice and trendy to see wonderfu...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23750 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text Class\n",
              "0      Before I begin, I want to briefly say that thi...   neg\n",
              "1      There has been a lot of love that has been put...   neg\n",
              "2      Hello I am from Denmark, and one day i was hav...   neg\n",
              "3      Shot into car from through the windscreen, som...   pos\n",
              "4      For all of the hype about this film, I kept an...   neg\n",
              "...                                                  ...   ...\n",
              "23745  \"Godzilla vs King Ghidorah\" is a perfect examp...   neg\n",
              "23746  This is one of those films that explore the cu...   pos\n",
              "23747  Ok I will sum up this movie... A bunch of skan...   neg\n",
              "23748  Wow-this one sucks. I'm gonna sum it up as qui...   neg\n",
              "23749  I suppose it's nice and trendy to see wonderfu...   neg\n",
              "\n",
              "[23750 rows x 2 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_labels_df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRBHRBr5zpo"
      },
      "source": [
        "With the AA_dict (words to Random unitary matrix dict), we create the feature vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJnx_luA5DHk",
        "outputId": "92d37f29-76ce-4bdb-ccd1-7ef7e72b87b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 23750/23750 [00:30<00:00, 766.34it/s]\n",
            "100%|██████████| 1250/1250 [00:01<00:00, 732.16it/s]\n",
            "100%|██████████| 25000/25000 [00:32<00:00, 768.11it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_fea_df = create_features_label_df(text_labels_df_train, AA_dict = AA_dict)\n",
        "valid_fea_df = create_features_label_df(text_labels_df_valid, AA_dict = AA_dict)\n",
        "test_fea_df  =  create_features_label_df(text_labels_df_test, AA_dict = AA_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Bp4zooDcG1fp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train pairs\n",
        "X_train_raw = train_fea_df.iloc[:, 2:]\n",
        "y_train = train_fea_df.iloc[:, 1]\n",
        "\n",
        "# Valid pairs\n",
        "X_valid_raw = valid_fea_df.iloc[:, 2:]\n",
        "y_valid = valid_fea_df.iloc[:, 1]\n",
        "\n",
        "# Test pairs\n",
        "X_test_raw = test_fea_df.iloc[:, 2:]\n",
        "y_test = test_fea_df.iloc[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "np_dZLuZG4R7"
      },
      "outputs": [],
      "source": [
        "# Standardize Input\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VBT3eHurG63U"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.fit_transform(X_train_raw)\n",
        "X_valid = scaler.transform(X_valid_raw)\n",
        "X_test = scaler.transform(X_test_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pVn7WotPks85"
      },
      "outputs": [],
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3UDD0II6PfB"
      },
      "source": [
        "This is a simple model with one hidden layer of NxN units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Y6xxCj2Cktrx"
      },
      "outputs": [],
      "source": [
        "class TextClassificationModelQM(nn.Module):\n",
        "\n",
        "    def __init__(self,  embed_dim, num_class):\n",
        "        super(TextClassificationModelQM, self).__init__()\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \n",
        "        return self.fc(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AvrbjqXfkxx2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(dataloader):\n",
        "    print(\"Training...\")\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for idx, (text, label) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "        running_loss += loss.item() *text.shape[0]\n",
        "\n",
        "    return running_loss\n",
        "    \n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (text, label) in enumerate(dataloader):\n",
        "            predited_label = model(text)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNjNei3bk5-r",
        "outputId": "e9aaafe9-a009-4878-9a8f-94916c5fb8ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "num_class = len(list(set(labels)))\n",
        "print(num_class)\n",
        "\n",
        "model = TextClassificationModelQM(N*N, num_class).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tEEEdTASk-NK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10         # epoch\n",
        "LR = 0.001           # learning rate\n",
        "BATCH_SIZE = 32     # batch size for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uyH6ZBx7lGDP"
      },
      "outputs": [],
      "source": [
        "# This is used for create the dataset loader\n",
        "\n",
        "class CustomTextDatasetNEW(Dataset):\n",
        "    def __init__(self, X_data, y_data):\n",
        "        \n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        target = label_pipeline(self.y_data[index])\n",
        "        \n",
        "        return self.X_data[index],  torch.tensor(target, dtype=torch.int64).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1o4_85c0lCDs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# define data set object\n",
        "\n",
        "TD_train = CustomTextDatasetNEW(\n",
        "    torch.FloatTensor(X_train).to(device),y_train)\n",
        "\n",
        "TD_valid = CustomTextDatasetNEW(\n",
        "    torch.FloatTensor(X_valid).to(device),y_valid)\n",
        "\n",
        "TD_test = CustomTextDatasetNEW(\n",
        "    torch.FloatTensor(X_test).to(device), y_test)\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(TD_train, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, )\n",
        "valid_dataloader = DataLoader(TD_valid, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False,)\n",
        "test_dataloader = DataLoader(TD_test, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCbvcW1w6Dsy"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeHDFzn3lPMA",
        "outputId": "313f086c-e234-4233-a0a2-088426ecb5f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n",
            "| epoch   1 |   500/  743 batches | accuracy    0.621\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  1.26s | valid accuracy    0.633 \n",
            "Epoch-1 lr: 0.001\n",
            "Epoch-1 loss: 31.43215329441697\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   2 |   500/  743 batches | accuracy    0.692\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  1.06s | valid accuracy    0.687 \n",
            "Epoch-2 lr: 0.001\n",
            "Epoch-2 loss: 20.617840633498226\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   3 |   500/  743 batches | accuracy    0.724\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  1.08s | valid accuracy    0.711 \n",
            "Epoch-3 lr: 0.0005\n",
            "Epoch-3 loss: 18.25298184260706\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   4 |   500/  743 batches | accuracy    0.738\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  1.08s | valid accuracy    0.715 \n",
            "Epoch-4 lr: 0.0005\n",
            "Epoch-4 loss: 17.31479905848548\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   5 |   500/  743 batches | accuracy    0.743\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  1.10s | valid accuracy    0.714 \n",
            "Epoch-5 lr: 0.0005\n",
            "Epoch-5 loss: 16.9866614892056\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   6 |   500/  743 batches | accuracy    0.743\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  1.08s | valid accuracy    0.719 \n",
            "Epoch-6 lr: 0.00025\n",
            "Epoch-6 loss: 16.804828107357025\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   7 |   500/  743 batches | accuracy    0.748\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  1.06s | valid accuracy    0.727 \n",
            "Epoch-7 lr: 0.00025\n",
            "Epoch-7 loss: 16.61151528783634\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   8 |   500/  743 batches | accuracy    0.746\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  1.08s | valid accuracy    0.730 \n",
            "Epoch-8 lr: 0.00025\n",
            "Epoch-8 loss: 16.557957067143068\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch   9 |   500/  743 batches | accuracy    0.756\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  1.08s | valid accuracy    0.727 \n",
            "Epoch-9 lr: 0.000125\n",
            "Epoch-9 loss: 16.494355055878334\n",
            "-----------------------------------------------------------\n",
            "Training...\n",
            "| epoch  10 |   500/  743 batches | accuracy    0.753\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  1.08s | valid accuracy    0.729 \n",
            "Epoch-10 lr: 0.000125\n",
            "Epoch-10 loss: 16.41809874636649\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3.0, gamma = 0.5)\n",
        "\n",
        "total_accu = None\n",
        "\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    running_loss = train(train_dataloader)\n",
        "    \n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "\n",
        "    #if total_accu is not None and total_accu > accu_val:\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    #else:\n",
        "\n",
        "    total_accu = accu_val\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
        "    \n",
        "    print('Epoch-{0} loss: {1}'.format(epoch, losses[-1]))\n",
        "\n",
        "    print('-' * 59)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyORvupyH0DkBgxXJisPDwpZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "QuantumFunctions_IMDB.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b38c1a484b1dd43bafeeca7a9beb45e4a9fd89a709f1072c4e620c4b4d81e7a7"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('ML_p38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
